---
title: "M5 Forecasting: Accuracy EDA"
author: "Kuldeep Singh Chouhan"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: false
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---

**"Learning by doing"** the motto here is to help beginners engage more with Kaggle Competitions and to motivate them to start doing **practical Learning**. I personally like to thank [Kaggle](https://www.kaggle.com/) for providing such a great platform. 

# Introduction

In this competition we are given a task to predict the sales for the next 28 days for **[Walmart](https://en.wikipedia.org/wiki/Walmart)** the **world's largest company by revenue.** We will use hierarchical sales data from Walmart at three US states of California, Texas, and Wisconsin. This is one of the two complementary competitions that together comprise the M5 forecasting challenge. This one is **M5 Forecasting Accuracy** and the other is **[M5 Forecasting Uncertainty](https://www.kaggle.com/c/m5-forecasting-uncertainty)** where our task is to estimate the uncertainity of our predicitions.

The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.


In this kernel I will try to perform some EDA to make some intuition about the data and the competition. 

# Loading Packages And Data {.tabset .tabset-fade .tabset-pills}

## Packages

```{r message=FALSE, warning=FALSE}
## Importing packages
library(tidyverse) 
library(data.table)
library(knitr)
library(kableExtra)
library(lubridate)
library(prophet)
library(plotly)
library(bbplot)
library(patchwork)
```

## Data

Loading our data.

```{r message=FALSE}
calendar <- read_csv("input/calendar.csv")
train <- fread("input/sales_train_validation.csv")
price <- fread("input/sell_prices.csv")
submission <- fread("input/sample_submission.csv")

```

# Exploring Data {.tabset .tabset-fade .tabset-pills}

The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy. We will look at each **.csv** to get some information or glimpse about the type of data we are working with.

## Calendar

**calendar.csv** - Contains the dates on which products are sold. The dates are in a yyyy/dd/mm format. This csv file consists of total 14 variables (features) and 1969 rows (observations). 

  * It has features such as weekday, year, date, month representing date. It also consists of 4 colums reprsenting event features (such as religious festivals, holidays, cultural etc) and three snaps colums for three states. 
  
  * **The Supplemental Nutrition Assistance Program (SNAP)** is the largest federal nutrition assistance program. SNAP provides benefits to eligible low-income individuals and families via an Electronic Benefits Transfer card. For more information you check [here](https://www.benefits.gov/benefit/361).
  
  * It consists of days column with total 1969 days. The date starts from 2011-01-29 to 2016-06-19 including the validation period of 28 days which is not included in the training data.

```{r}
calendar[1:5,] %>%
  kable(format = "html") %>%
  kable_styling()

```

## Sales Training Data

**sales_train.csv** - Contains the historical daily unit sales data per product and store [d_1 - d_1913]. 

* This is the main training data. We can see there are columns containing information such as item, department, category, store and state and date variables for representing sales per date. 

* It has Columns starting with **d_** prefix which are dates indicating sales per day ranging from d_1 to d_1913, mean total 1913 days from 2011-01-29 to 2016-04-24. After including evaluation data of 28 days it will have total 1941 days which will be available once month before competition deadline.
  
* Most of the observations are from the state of **California** followed by **Texas & Wisconsin.**
  
```{r}
train[1:5, 1:10] %>%
  kable() %>%
  kable_styling()
```

* There are 10 stores in 3 states. 4 stores in California, 3 in Texas and 3 in Wisconsin. Also there are 3049 individual products across all stores.
  
```{r}
train %>%
  group_by(store_id, state_id) %>%
  count() %>%
  kable() %>%
  kable_styling()
```

* There are 7 departments with 3 different categories.
   
```{r}
train %>%
  group_by(dept_id, cat_id) %>%
  count() %>%
  kable() %>%
  kable_styling()
```

## Price Data

**sell_prices.csv** - the store and item IDs together with the sales price of the item as a weekly average.


```{r}
price[1:10,] %>%
  kable() %>%
  kable_styling()
```

# Preparing Data for Time Series {.tabset .tabset-fade .tabset-pills}

Our data is not in right format. We need to do some manipulation and preprocessing to visualize it. Our data is wide we need to transform it into long format with a date column since in order to plot it using **ggplot2**. Always check with str(data) how variables are understood by R. If not read as a date, use [lubridate](https://www.r-graph-gallery.com/time-series.html) to convert it. In order to develop some inuition about how to visualize time series data I took help from [here](https://www.r-graph-gallery.com/279-plotting-time-series-with-ggplot2.html).

## Glimpse

Let us get a quick glimpse of data types of respective columns in our **sales_train_validation.csv**. We can clearly see that there isn't any **date** column. We need to create a **Date Object** first.

```{r}
sapply(train[1:5, 1:10], class) %>%
  kable() %>%
  kable_styling()
```


## Helper function 

Thanks to [Martin Henze](https://www.kaggle.com/headsortails) for this helper function. We are going to use [tidyverse](https://www.tidyverse.org/) for this. We are going to creating **date object** with the help of [lubridate's](https://lubridate.tidyverse.org/) **date()** and particularly using [pivot_longer](https://tidyr.tidyverse.org/reference/pivot_longer.html) to increase the number of rows and decrease the number of columns.

```{r}
extract_ts <- function(df){
  
  min_date <- date("2011-01-29")
  
  df %>%
    select(id, starts_with("d_")) %>%  
    pivot_longer(starts_with("d_"), names_to = "dates", values_to = "sales") %>%
    mutate(dates = as.integer(str_remove(dates, "d_"))) %>% 
    mutate(dates = min_date + dates - 1) %>% 
    mutate(id = str_remove(id, "_validation"))
}

```

# Exploratory Data Analysis: Time Series

We will now start our journey by visualizing our data.

## Aggregate Sales

```{r}
foo <- train %>%
  summarise_at(vars(starts_with("d_")), sum) %>%
  mutate(id = 1)

bar <- extract_ts(foo)

gg <- bar %>%
  ggplot(aes(dates, sales)) +
  geom_line(col = "blue") +
  bbc_style() +
  labs(x = "Date", y = "Sales", title = "All Aggregate Sales")

ggplotly(gg, dynamicTicks = TRUE)

```

  * The sales are increasing over time. 
  
## Sales Per Staet

```{r}
foo <- train %>%
  group_by(state_id) %>%
  summarise_at(vars(starts_with("d_")), sum) %>%
  rename(id = state_id)

bar <- extract_ts(foo) %>% 
  mutate(month = month(dates),
         year = year(dates)) %>% 
  group_by(month, year, id) %>% 
  summarise(sales = sum(sales),
            dates = min(dates)) %>% 
  ungroup() %>% 
  filter(str_detect(as.character(dates), "..-..-01")) %>% 
  filter(dates != max(dates))

gg <- bar %>% 
  ggplot(aes(dates, sales, col = id)) +
  geom_line() +
  bbc_style() +
  labs(x = "Date", y = "Sales", title = "Monthly Sales per State")

ggplotly(gg, dynamicTicks = TRUE)

```

```{r warning=FALSE}

foo <- train %>%
  group_by(cat_id) %>% 
  summarise_at(vars(starts_with("d_")), sum) %>% 
  rename(id = cat_id)

bar <- train %>%
  group_by(store_id) %>% 
  summarise_at(vars(starts_with("d_")), sum) %>% 
  rename(id = store_id)

p1 <- extract_ts(foo) %>% 
  mutate(month = month(dates),
         year = year(dates)) %>% 
  group_by(month, year, id) %>% 
  summarise(sales = sum(sales),
            dates = min(dates)) %>% 
  ungroup() %>% 
  filter(str_detect(as.character(dates), "..-..-01")) %>% 
  filter(dates != max(dates)) %>% 
  ggplot(aes(dates, sales, col = id)) +
  geom_line() +
  bbc_style() +
  theme(legend.position = "none") +
  labs(title = "Sales per Category", x = "Date", y = "Sales")

p2 <- train %>% 
  count(cat_id) %>% 
  ggplot(aes(cat_id, n, fill = cat_id)) +
  geom_col() +
  bbc_style() +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(size = 7)) +
  labs(x = "", y = "", title = "Rows per Category")

p3 <- extract_ts(bar) %>% 
  mutate(month = month(dates),
         year = year(dates)) %>% 
  group_by(month, year, id) %>% 
  summarise(sales = sum(sales),
            dates = min(dates)) %>% 
  ungroup() %>% 
  filter(str_detect(as.character(dates), "..-..-01")) %>% 
  filter(dates != max(dates)) %>% 
  mutate(state_id = str_sub(id, 1, 2)) %>% 
  ggplot(aes(dates, sales, col = id)) +
  geom_line() +
  bbc_style() +
  theme(legend.position = "bottom") +
  labs(title = "Sales per Store", x = "Date", y = "Sales", col = "Store ID") +
  facet_wrap(~state_id)

layout <- "
AAB
CCC
"

p1 + p2 + p3 + plot_layout(design = layout)

```

# Explanatory Variables

# Reference

* [pivot_longer](https://tidyr.tidyverse.org/reference/pivot_longer.html)

* [r-graph-gallery](https://www.r-graph-gallery.com/279-plotting-time-series-with-ggplot2.html) 